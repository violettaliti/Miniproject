Thi Hoang | 14.10.2025

# Thi's Miniproject â‚^. .^â‚âŸ†

## Project goal:

To carry out a mini project and datapipeline using `APIs`, `web scraping`, `Docker`, `Git`, databases (`PostgreSQL`, `pgAdmin4`) and data visualisation tool (`Power BI`).

***Topic***: still brainstorming.

## Project flow

- [ ] Choose a topic /â€¢á·…â€â€â€¢á·„\à©­ â¯â¯â¯â¯ choose relevant APIs
- [ ] Dockerise the process â¯â¯â¯â¯ `Dockerfile` + `Dockerimage` + `Dockercompose`
  - [ ] Docker prototype
- [ ] Retrieve data from `APIs` â¯â¯â¯â¯ redefine my project goals based on the available and potential data retrieved
  - [ ] API prototype
- [ ] Scrape complementary data from `websites` â¯â¯â¯â¯ see the list below
  - [ ] Web scraper prototype
- [ ] Store the acquired data in `PostgreSQL` â¯â¯â¯â¯ `pgAdmin4` 
  - [ ] Database prototype
- [ ] Clean, transform and export data for visualisation â¯â¯â¯â¯ `Power BI` 
- [ ] Connect `Power BI` to `PostgreSQL` database container
- [ ] Play with visualisation ğŸ“Š
- [ ] Tidy up Git (e.g. API keys)
- [ ] Write AI models to play with the processed data (mostly supervised learning)
  - [ ] AI models prototype
- [ ] Wrap up the project ^. .^â‚à¸…

## Project details:

- APIs:

- Websites:

## Project structure:
```bash
miniproject/
â”œâ”€ README.md
â”œâ”€ .gitignore
â”œâ”€ .env # to store API_KEYs
â”œâ”€ requirements.txt
â”œâ”€ docker-compose.yml
â”œâ”€ Dockerfile
â”œâ”€ src/
â”‚  â”œâ”€ __init__.py
â”‚  â”œâ”€ api_logger.py # APIs (requests)
â”‚  â”œâ”€ web_logger.py # web scraper (requests + BeautifulSoup)
â”‚  â”œâ”€ exporter.py # export to sql
â”‚  â””â”€ tests/ # unittests
â”‚     â”œâ”€ __init__.py
â”‚     â”œâ”€ test_api_logger.py
â”‚     â”œâ”€ test_web_logger.py
â”‚     â””â”€ test_exporter.py
â””â”€ postgres_data/
   â”œâ”€ db/
   â””â”€ init/
     â””â”€ schema.sql # DDL in 3Nf
```